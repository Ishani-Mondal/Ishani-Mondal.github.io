<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Ishani Mondal</title>

    <meta name="author" content="Ishani Mondal">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      
      <tr style="padding:0px">
        <td  style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr class="highlighted-row" style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Ishani Mondal
                </p>
                <p>
                  I am a third-year PhD student in Computer Science at the <a href="https://wiki.umiacs.umd.edu/clip/index.php/Main_Page">CLIP Lab</a> , University of Maryland, College Park, advised by Prof. <a href="https://users.umiacs.umd.edu/~ying/">Jordan Lee Boyd-Graber</a>. I am fortunate to work in close collaborations with Prof. <a href="https://rudinger.github.io/">Rachel Rudinger</a> and Prof. <a href="https://tianyizhou.github.io/">Tianyi Zhou</a>.
                  My research interests span human-centered natural language processing, multimodal reasoning, cognitive evaluation of AI systems, and personalized language generation. I have worked on aligning AI outputs with human intent, group preferences, and cognitive constructs such as Theory of Mind. Ishani has previously held research positions at Microsoft Research, Adobe Research, IBM Research UK, TCS Research, and IIT Kharagpur, contributing to projects at the intersection of language, reasoning, and social impact.  
                </p>
                
                <p style="text-align:center">
                  <a href="mailto:imondal@umd.edu">Email</a> &nbsp;/&nbsp;
                  <a href="Ishani_Resume_Updated.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="bio.txt">Bio</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.co.in/citations?user=MAhGgrEAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <!--<a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp;-->
                  <a href="https://www.linkedin.com/in/ishani-mondal-90ba39b0/">Linkedin</a> &nbsp;/&nbsp;
                  <a href="https://github.com/Ishani-Mondal/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="19728142.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="19728142.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr class="highlighted-row">
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in natural language processing, generative AI, and cognitive reasoning. Most of my research is about connecting the dots between human and AI's power to solve interesting tasks and evaluate them from a cognitive perspective.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <h2>List of Publications</h2>
            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src='GPA.jpg' width=100%>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://openreview.net/forum?id=xkgfLXZ4e0">
                  <span class="papertitle">Group Preference Alignment: Customized LLM Response Generation from In-Situ Conversations</span>
                </a>
                <br>
                <strong>Ishani Mondal</strong>,
                <a href="https://github.com/ArthurBrussee">Jay Stokes</a>,
                <a href="https://holynski.org">Sujay Kumar Jauhar</a>,
                <a href="https://ricardomartinbrualla.com">Longqi Yang</a>,
                <a href="https://henzler.github.io">Mengting Wan</a>
                <a href="https://ricardomartinbrualla.com">Xiaofeng Xu</a>,
                <a href="https://henzler.github.io">Xia Song</a>,
                <a href="https://henzler.github.io">Jordan Lee Boyd-Graber</a>,
                <a href="https://ricardomartinbrualla.com">Jennifer Neville</a>
                <br>
                <em>Under Review at EMNLP</em>, 2025
                <br>
                <a href="https://arxiv.org/abs/2310.11523">ArXiv</a>
                <p></p>
                <p>
                  LLMs often fail to meet the specialized needs of distinct user groups due to their one-size-fits-all training paradigm and there is limited research on what personalization aspects each group expect. To address these limitations, we propose a group-aware personalization framework, Group Preference Alignment (GPA), that identifies context-specific variations in conversational preferences across user groups and then steers LLMs to address those preferences. Our approach consists of two steps: (1) Group-Aware Preference Extraction, where maximally divergent user-group preferences are extracted from real-world conversation logs and distilled into interpretable rubrics, and (2) Tailored Response Generation, which leverages these rubrics through two methods: a) Context-Tuned Inference (GPA-CT), that dynamically adjusts responses via context-dependent prompt instructions, and b) Rubric-Finetuning Inference (GPA-FT), which uses the rubrics to generate contrastive synthetic data for personalization of group-specific models via alignment. Experiments demonstrate that our framework significantly improves alignment of the output with respect to user preferences and outperforms baseline methods, while maintaining robust performance on standard benchmarks.
                </p>
              </td>
            </tr>


            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src='personad2s.jpg' width=100%>
              </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://openreview.net/forum?id=xkgfLXZ4e0">
          <span class="papertitle">Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)</span>
        </a>
        <br>
        <a href="https://szymanowiczs.github.io/">SUBBA REDDY OOTA</a>,
        <a href="https://jasonyzhang.com">Akshett Rai Jindal </a>,
        <strong>Ishani Mondal</strong>,
        <a href="https://ruiqigao.github.io">Khushbu Pahwa</a>,
        <a href="https://github.com/ArthurBrussee">Satya Sai Srinath Namburi</a>,
        <a href="https://holynski.org">Manish Shrivastava </a>,
        <a href="https://ricardomartinbrualla.com">Maneeesh Singh</a>,
        <a href="https://henzler.github.io">Raju Surampudi Bap</a>
        <a href="https://ricardomartinbrualla.com">Manish Gupta</a>,
        <br>
        <em>ICLR</em>, 2025
        <br>
        <a href="https://iclr.cc/media/PosterPDFs/ICLR%202025/27766.png?t=1743588045.258231">Poster</a>
        /
        <a href="https://arxiv.org/html/2505.20029v1">ArXiv</a>
        /
        <a href="https://github.com/subbareddy248/mllm_instruction_brain">Github</a>
        <p></p>
        <p>
          Instruction-tuned multimodal LLMs (MLLMs) show stronger alignment with brain activity during natural scene viewing compared to vision-only models, especially when processing task-specific instructions like image captioning and visual question answering. However, not all instructions contribute equally to brain alignment, highlighting the need for more precise instruction encoding to better predict neural responses.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src='advscore.jpg' width=100%>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://aclanthology.org/2025.naacl-long.27.pdf">
			<span class="papertitle">Is your benchmark truly adversarial? AdvScore: Evaluating Human-Grounded Adversarialness
</span>
        </a>
        <br>
				<a href="https://www.cs.columbia.edu/~rundi/">Yoo Yeon Sung</a>,
				<a href="https://ruiqigao.github.io/">Maharshi Gor</a>,
				<a href="https://poolio.github.io/">Eve Flesig</a>,
				<strong>Ishani Mondal</strong>,
				<a href="https://www.cs.columbia.edu/~cxz/index.htm/">Jordan Lee Boyd-Graber</a>
        <br>
        <em>NAACL</em>, 2025 &nbsp <font color="red"><strong>(Outstanding Paper Award)</strong></font>
        <br>
        <a href="https://github.com/yysung/Advscore">Code</a>
        <p></p>
        <p>
          Adversarial datasets should validate AI robustness by providing samples on which humans perform well, but models do not. However, as models evolve, datasets can become obsolete. Measuring whether a dataset remains adversarial is hindered by the lack of a standardized metric for measuring adversarialness. We propose ADVSCORE, a human-grounded evaluation metric that assesses a dataset’s adversarialness by capturing models’ and humans’ varying abilities, while also identifying poor examples. We then use ADVSCORE to motivate a new dataset creation pipeline for realistic and high-quality adversarial samples, enabling us to collect an adversarial question answering (QA) dataset, ADVQA. We apply ADVSCORE using 9,347 human responses and ten language models’ predictions to track model improvement over five years (2020–2024). ADVSCORE thus provides guidance for achieving robustness comparable with human capabilities. Furthermore, it helps determine to what extent adversarial datasets continue to pose challenges, ensuring that, rather than reflecting outdated or overly artificial difficulties, they effectively test model capabilities.
        </p>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src='adaptiveie.jpg' width=100%>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://aclanthology.org/2025.coling-main.392/">
          <span class="papertitle">ADAPTIVE IE: Investigating the Complementarity of Human-AI Collaboration to Adaptively Extract Information on-the-fly</span>
        </a>
        <br>
        <strong>Ishani Mondal</strong>,
        <a href="https://henzler.github.io/">Michelle Yuan</a>,
        <a href="https://dorverbin.github.io/">Anandhavelu Natarajan</a>
        <a href="https://jbhuang0604.github.io/">Aparna Garimella</a>,
        <a href="https://pratulsrinivasan.github.io/">Francis Ferraro</a>, 
        <a href="https://dorverbin.github.io/">Andrew Blair-Stanek</a>
        <a href="https://dorverbin.github.io/">Benjamin Van Durme</a>,
        <a href="https://dorverbin.github.io/">Jordan Lee Boyd-Graber</a>
        <br>
        <em>COLING</em>, 2025 &nbsp <font color=#FF8080><strong>(Oral)</strong></font>
        <br>
        <a href="https://relight-to-reconstruct.github.io/">Paper</a>
        /
        <a href="https://arxiv.org/abs/2412.15211">Slides</a>
        /
        <a href="https://arxiv.org/abs/2412.15211">Video</a>
        <p></p>
        <p>
          Information extraction (IE) needs vary over time, where a flexible information extraction (IE) system can be useful. Despite this, existing IE systems are either fully supervised, requiring expensive human annotations, or fully unsupervised, extracting information that often do not cater to user’s needs. To address these issues, we formally introduce the task of “IE on-the-fly”, and address the problem using our proposed Adaptive IE framework that uses human-in-the-loop refinement to adapt to changing user questions. Through human experiments on three diverse datasets, we demonstrate that Adaptive IE is a domain-agnostic, responsive, efficient framework for helping users access useful information while quickly reorganizing information in response to evolving information needs.
        </p>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src='diagram.jpg' width=100%>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://aclanthology.org/2024.findings-emnlp.780/">
          <span class="papertitle">SciDoc2Diagrammer-MAF: Towards Generation of Scientific Diagrams
            from Documents guided by Multi-Aspect Feedback Refinement</span>
        </a>
        <br>
        <strong>Ishani Mondal</strong>,
        <a href="https://henzler.github.io/">Zongxia Li</a>,
        <a href="https://dorverbin.github.io/">Yufang Hou</a>
        <a href="https://pratulsrinivasan.github.io/">Anandhavelu Natarajan</a>,
        <a href="https://jbhuang0604.github.io/">Aparna Garimella</a>, 
        <a href="https://dorverbin.github.io/">Jordan Lee Boyd-Graber</a>
        <br>
        <em>EMNLP</em>, 2024 &nbsp 
        <br>
        <a href="https://aclanthology.org/2024.findings-emnlp.780/">Paper</a>
        /
        <a href="https://arxiv.org/abs/2412.15211">Slides</a>
        /
        <a href="https://arxiv.org/abs/2412.15211">Video</a>
        <p></p>
        <p>
          Automating the creation of scientific diagrams from academic papers can significantly streamline the development of tutorials, presentations, and posters, thereby saving time and accelerating the process. Current text-to-image models (Rombach et al., 2022a; Belouadi et al., 2023) struggle with generating accurate and visually appealing diagrams from long-context inputs. We propose SciDoc2Diagram, a task that extracts relevant information from scientific papers and generates diagrams, along with a benchmarking dataset, SciDoc2DiagramBench. We develop a multi-step pipeline SciDoc2Diagrammer that generates diagrams based on user intentions using intermediate code generation. We observed that initial diagram drafts were often incomplete or unfaithful to the source, leading us to develop SciDoc2Diagrammer-Multi-Aspect-Feedback (MAF), a refinement strategy that significantly enhances factual correctness and visual appeal and outperforms existing models on both automatic and human judgement.
        </p>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src='personad2s.jpg' width=100%>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://aclanthology.org/2024.eacl-long.163/">
          <span class="papertitle">Presentations by the Humans and For the Humans: Harnessing LLMs for Generating Persona-Aware Slides from Documents</span>
        </a>
        <br>
        <strong>Ishani Mondal</strong>,
        <a href="https://henzler.github.io/">Shweta Somasamundaram</a>,
        <a href="https://dorverbin.github.io/">Anandhavelu Natarajan</a>
        <a href="https://jbhuang0604.github.io/">Aparna Garimella</a>,
        <a href="https://pratulsrinivasan.github.io/">Sambaran Bhattacharya</a>, 
        <a href="https://dorverbin.github.io/">Jordan Lee Boyd-Graber</a>
        <br>
        <em>EACL</em>, 2024 &nbsp <font color=#FF8080><strong>(Oral)</strong></font>
        <br>
        <a href="https://aclanthology.org/2024.eacl-long.163/">Paper</a>
        /
        <a href="https://arxiv.org/abs/2412.15211">Slides</a>
        /
        <a href="https://aclanthology.org/2024.eacl-long.163.mp4">Video</a>
        <p></p>
        <p>
          Scientific papers and slides are two different representations of the same underlying information, but both require substantial work to prepare. While there had been prior efforts on automating document-to-slides generation, there is still a pressing need of customizing the presentation of content aligning with the persona of target audience or duration of presentation. This paper first introduces the concept of end-user specification-aware document to slides conversion that incorporates end-user specifications into the conversion process. For this, we initially introduce a new dataset reuse the existing SciDuet dataset consisting of pairs of papers and corresponding slides decks from recent years’ *ACL conferences to create four persona-aware configurations. Secondly, we present Persona-Aware-D2S, a novel approach by finetuning LLMs using target audience feedback to create persona-aware slides from scientific documents. Our evaluation on both automated metrics and qualitative human evaluation suggests that by incorporating end-user specifications into the conversion process, our model can create presentations that are not only informative but also tailored to expectations and cognitive abilities of target audience.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src='pedants.jpg' width=100%>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://aclanthology.org/2024.findings-emnlp.548.pdf">
          <span class="papertitle">PEDANTS: Cheap but Effective and Interpretable Answer Equivalence</span>
        </a>
        <br>
        <a href="https://alextrevithick.github.io/">Zongxia Li</a>,
        <strong>Ishani Mondal</strong>,
        <a href="https://henzler.github.io/">Huy Nghiem</a>,
        <a href="https://dorverbin.github.io/">Yijun Liang</a>,
        <a href="https://www.cs.columbia.edu/~rundi/">Jordan Lee Boyd-Graber</a>
        <br>
        <em>EMNLP</em>, 2024
        <br>
        <a href="https://arxiv.org/abs/2402.11161">arXiv</a>
        /
        <a href="https://github.com/zli12321/PEDANTS-LLM-Evaluation">Code</a>
        <p></p>
        <p>
          Question answering (QA) can only make progress if we know if an answer is correct, but current answer correctness (AC) metrics struggle with verbose, free-form answers from large language models (LLMs). There are two challenges with current short-form QA evaluations: a lack of diverse styles of evaluation data and an over-reliance on expensive and slow LLMs. LLM-based scorers correlate better with humans, but this expensive task has only been tested on limited QA datasets. We rectify these issues by providing rubrics and datasets for evaluating machine QA adopted from the Trivia community. We also propose an efficient, and interpretable QA evaluation that is more stable than an exact match and neural methods (BERTScore).
        </p>
      </td>
    </tr>
    



          </tbody></table>

          
					<!-- <table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
           
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #fcb97d;">
								 <h2>Micropapers</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                <br>
                <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
                <br>
                <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                <br>
                <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How Can Academics Adapt?</a>
              </td>
            </tr>


            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #aaba9e;">
								 <h2>Recorded Talks</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="https://www.youtube.com/watch?v=hFlF33JZbA0">Radiance Fields and the Future of Generative Media, 2025</a><br>				  
                <a href="https://www.youtube.com/watch?v=h9vq_65eDas">View Dependent Podcast, 2024</a><br>
                <a href="https://www.youtube.com/watch?v=4tDhYsFuEqo">Bay Area Robotics Symposium, 2023
</a><br>
                <a href="https://youtu.be/TvWkwDYtBP4?t=7604">EGSR Keynote, 2021</a><br>
				<a href="https://www.youtube.com/watch?v=nRyOzHpcr4Q">TUM AI Lecture Series, 2020</a><br>
				<a href="https://www.youtube.com/watch?v=HfJpQCBTqZs">Vision & Graphics Seminar at MIT, 2020</a>
              </td>
            </tr>

            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #c6b89e;">
								 <h2>Academic Service</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="https://iccv.thecvf.com/">Lead Area Chair, ICCV 2025</a>
                <br>
                <a href="https://cvpr.thecvf.com/Conferences/2025/Organizers">Lead Area Chair, CVPR 2025</a>
                <br>
                <a href="https://cvpr.thecvf.com/Conferences/2024/Organizers">Area Chair, CVPR 2024</a>
                <br>
                <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
                <br>
                <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                <br>
                <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Award Committee Member, CVPR 2021</a>
                <br>
                <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                <br>
                <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
              </td>
            </tr>
						
						
           
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #edd892;">
								 <h2>Teaching</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
                <br>
                <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
                <br>
                <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
              </td>
            </tr>
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table> -->
  </body>
</html>
